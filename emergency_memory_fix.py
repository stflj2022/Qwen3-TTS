#!/usr/bin/env python3
"""
ç´§æ€¥å†…å­˜æ³„æ¼ä¿®å¤è„šæœ¬
è§£å†³åƒé—®è¯­éŸ³å…‹éš†å†…å­˜æ³„æ¼é—®é¢˜
"""

import os
import sys
import time
import gc
import torch
import psutil
from pathlib import Path

def check_memory_usage():
    """æ£€æŸ¥å½“å‰å†…å­˜ä½¿ç”¨"""
    process = psutil.Process()
    memory_info = process.memory_info()
    memory_gb = memory_info.rss / (1024 ** 3)
    return memory_gb, memory_info

def force_memory_cleanup():
    """å¼ºåˆ¶å†…å­˜æ¸…ç†"""
    print("ğŸ§¹ æ‰§è¡Œç´§æ€¥å†…å­˜æ¸…ç†...")
    
    # æ¸…ç†Pythonåƒåœ¾
    collected = gc.collect()
    print(f"Python GCå›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
    
    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        print("GPUç¼“å­˜å·²æ¸…ç†")
    
    # å†æ¬¡åƒåœ¾å›æ”¶
    collected = gc.collect()
    print(f"ç¬¬äºŒæ¬¡GCå›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
    
    return True

def check_model_memory():
    """æ£€æŸ¥æ¨¡å‹å†…å­˜å ç”¨"""
    try:
        # æ£€æŸ¥PyTorchæ¨¡å‹
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / (1024 ** 3)
                cached = torch.cuda.memory_reserved(i) / (1024 ** 3)
                print(f"GPU {i}: åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")
        
        # æ£€æŸ¥å¤§å¯¹è±¡
        large_objects = []
        for obj in gc.get_objects():
            if hasattr(obj, '__sizeof__'):
                try:
                    size = sys.getsizeof(obj) / (1024 ** 2)
                    if size > 0.1:  # å¤§äº100MBçš„å¯¹è±¡
                        large_objects.append((type(obj).__name__, size))
                except:
                    pass
        
        if large_objects:
            print("âš ï¸ å‘ç°å¤§å¯¹è±¡:")
            for obj_type, size in sorted(large_objects, key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {obj_type}: {size:.2f}GB")
        
    except Exception as e:
        print(f"æ£€æŸ¥æ¨¡å‹å†…å­˜æ—¶å‡ºé”™: {e}")

def restart_application():
    """é‡å¯åº”ç”¨"""
    print("ğŸ”„ æ­£åœ¨é‡å¯åº”ç”¨...")
    
    # æ¸…ç†æ‰€æœ‰å¯èƒ½çš„æ®‹ç•™
    force_memory_cleanup()
    
    # ç»™ç”¨æˆ·é‡å¯å»ºè®®
    print("\nğŸ’¡ åº”ç”¨å·²å®‰å…¨é€€å‡ºï¼Œè¯·é‡æ–°å¯åŠ¨:")
    print("cd /home/wu/æ–‡æ¡£/åƒé—®è¯­éŸ³å…‹éš†")
    print("./å¯åŠ¨WebUI.sh")
    print("\nğŸ”§ å¦‚æœä»æœ‰é—®é¢˜ï¼Œå»ºè®®:")
    print("1. é‡å¯ç”µè„‘æ¸…ç†å†…å­˜")
    print("2. æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å†…å­˜å¯†é›†å‹ç¨‹åº")
    print("3. è€ƒè™‘å¢åŠ ç³»ç»Ÿè™šæ‹Ÿå†…å­˜")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš¨ åƒé—®è¯­éŸ³å…‹éš† - ç´§æ€¥å†…å­˜æ³„æ¼ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥å½“å‰å†…å­˜çŠ¶æ€
    current_memory, memory_info = check_memory_usage()
    print(f"ğŸ“Š å½“å‰å†…å­˜ä½¿ç”¨: {current_memory:.2f}GB")
    
    # æ£€æŸ¥æ¨¡å‹å†…å­˜å ç”¨
    print("ğŸ” æ£€æŸ¥æ¨¡å‹å†…å­˜å ç”¨...")
    check_model_memory()
    
    # å¼ºåˆ¶å†…å­˜æ¸…ç†
    force_memory_cleanup()
    
    # å†æ¬¡æ£€æŸ¥å†…å­˜
    after_memory, _ = check_memory_usage()
    memory_freed = current_memory - after_memory
    
    print(f"âœ… å†…å­˜æ¸…ç†å®Œæˆ:")
    print(f"   æ¸…ç†å‰: {current_memory:.2f}GB")
    print(f"   æ¸…ç†å: {after_memory:.2f}GB")
    print(f"   é‡Šæ”¾å†…å­˜: {memory_freed:.2f}GB")
    
    # å¦‚æœå†…å­˜ä»ç„¶è¿‡é«˜ï¼Œå»ºè®®é‡å¯
    if after_memory > 6.0:  # è¶…è¿‡6GBå»ºè®®é‡å¯
        print(f"\nâš ï¸ å†…å­˜ä½¿ç”¨ä»ç„¶è¿‡é«˜: {after_memory:.2f}GB")
        restart_application()
        return False
    
    print("\nğŸ’¾ å†…å­˜ä½¿ç”¨å·²é™è‡³å®‰å…¨èŒƒå›´ï¼Œå¯ä»¥ç»§ç»­ä½¿ç”¨")
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)